<!DOCTYPE html>
<html lang="en">
  <!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="http://j-collins.appspot.com/favicon.ico">

    <title>Jasmine L. Collins</title>

    <!-- Bootstrap core CSS -->
    <link href="http://j-collins.appspot.com/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="http://j-collins.appspot.com/assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="http://j-collins.appspot.com/css/jumbotron-narrow.css" rel="stylesheet">
    <link href="http://j-collins.appspot.com/css/style.css" rel="stylesheet">

    <script src="http://j-collins.appspot.com/assets/js/ie-emulation-modes-warning.js"></script>
    
    <!-- jQuery -->
    <script type="text/javascript" src="http://code.jquery.com/jquery-1.9.1.js"></script>
    
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

  </head>

  <body>

    <div class="container">
      <div class="header clearfix">
        <nav>
          <ul class="nav nav-pills pull-right">
            <li role="presentation"><a href="index.html">Home</a></li>
            <li class="dropdown">
                <a class="dropdown-toggle" data-toggle="dropdown" href="#">Projects
                <span class="caret"></span></a>
                <ul class="dropdown-menu">
                  <li><a href="deepdream.html">Refining images with DeepDream</a></li>
                  <li><a href="koes.html">Drug discovery meets ML</a></li>
                  <li><a href="molecule.html">Molecular visualization</a></li>
                  <li><a href="neural.html">Neural reward in children</a></li> 
                </ul>
              </li>
            <li role="presentation"><a href="coursework.html">Coursework</a></li>
            <li role="presentation"><a href="hobbies.html">Hobbies</a></li>
          </ul>
        </nav>
        <h4 class="text-muted">Jasmine L. Collins</h4><h5 class="text-muted">jlcollins121@gmail.com</h5>
      </div>
      
      

<h3>Refining Images using DeepDream Inceptionism</h3>
  <p>
    For my Machine Learning class project, I decided to look at whether or not we can 
    refine images by using techniques for visualizing what a Convolutional Neural Network 
    (CNN) trained on an image recognition task has learned. The class project was inspired 
    by <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html" target="_blank">this</a> 
    blog post from the Google Research Blog.
    <br><br>
    <b>DeepDream</b><br>
    In DeepDream, an image input is fed through a trained network in a forward pass to 
    a specified layer, where the gradients are then 
    set equal to the activations themselves. Backprop is performed and then rather than 
    using the gradients to update the weights (as is typically done), the gradient values 
    are used to make an update to the image input. At a high level, this is like asking 
    the neural network to amplify the features that it "sees". 
    
    <br><br>
    <b>Activity maximization</b><br>
    Creating maximally activating class images is just a small departure from DeepDream. 
    Rather than performing a forward pass to any arbitrary layer, maximally activating 
    class images are generated by targeting the layer right before class prediction, and 
    rather than setting the gradients equal to the activations, only the gradient of the 
    class of interest is set to 1 and the rest are zeroed out. As before, backpropogation 
    is performed and the image is updated, rather than the weights. In effect, this 
    produces what the CNN would “like to see” when classifying a specific class.
    <br><br>
    In the blog post, Mordvintsev, Olah, and Tyka show various results of both DeepDream 
    and the activity maximization method. 
    They show the transformation of random noise into an image that maximally activates 
    a specified class (for example, "banana"). They also show the transformation of one 
    class into another (turning a tree into a building). This led me to wonder if this 
    technique could be used to refine images. That is, starting with an image from a 
    particular class - can we make the image look even more like that class?
    
    <br><br>
    <b>Constraints and regularizers</b><br>
    It is important to set the right constraints in order to generate outputs that 
    resemble natural 
images. For example in natural images, neighboring pixels are 
    typically correlated. To understand the need for constraints, I tried a few 
    well-known techniques from the literature. Specifically, I looked at the effects of
    <a href="https://arxiv.org/pdf/1312.6034.pdf" target="_blank">L2 regularization</a>, 
    <a href="https://arxiv.org/pdf/1506.06579v1.pdf" target="_blank">Gaussian blur</a>, and
    <a href="https://arxiv.org/pdf/1512.02017v3.pdf" target="_blank">jitter/pixel clipping</a>.
    
    
    
    
    <br><br>
    
    <center><figure>
    <img src="http://j-collins.appspot.com/images/mlproj/nothing_swan.jpg" height="200" width="200">
    <img src="http://j-collins.appspot.com/images/mlproj/just_clip.jpg" height="200" width="200">
    <img src="http://j-collins.appspot.com/images/mlproj/just_jitter.jpg" height="200" width="200">
    <img src="http://j-collins.appspot.com/images/mlproj/just_blur.jpg" height="200" width="200">
    <img src="http://j-collins.appspot.com/images/mlproj/all.jpg" height="200" width="200">
    <figcaption> Maximally activating image for Black Swan class, starting from noise. From left to right: 
    No constraints, only clipping, only jitter, only Gaussian blur, all methods combined.</figcaption>
    </figure></center>
    
    
    <br><br>
    <b>Refining images</b><br>
    Now rather than starting from noise, I wanted to see the results of an image from a 
    class, modified to look even more like an image from that class.
    <br><br>
    <center><figure>
    <img src="http://j-collins.appspot.com/images/mlproj/start_swan.jpg" height="200" width="200">
    <img src="http://j-collins.appspot.com/images/mlproj/100_iter.jpg" height="200" width="200">
    <img src="http://j-collins.appspot.com/images/mlproj/500_iter.jpg" height="200" width="200">
    <img src="http://j-collins.appspot.com/images/mlproj/1000_iter.jpg" height="200" width="200">
    <img src="http://j-collins.appspot.com/images/mlproj/5000_iter.jpg" height="200" width="200">
    <figcaption> The effect of increasing number of iterations in activity 
    maximization for the Black Swan class. From left to right: Original image, 100 
    iterations, 500 iterations, 1000 iterations, 5000 iterations.</figcaption>
    </figure></center>
    <br><br>
    <center><figure>
    <img src="http://j-collins.appspot.com/images/mlproj/flamingo.jpg" height="200" width="200">
    <img src="http://j-collins.appspot.com/images/mlproj/flamingo_100_iter.jpg" height="200" width="200">
    <img src="http://j-collins.appspot.com/images/mlproj/flamingo_1000_iter.jpg" height="200" width="200">
    <figcaption> The effect of increasing number of iterations in activity 
    maximization for the Flamingo class. From left to right: Original image, 100 
    iterations, 1000 iterations.</figcaption>
    </figure></center>
    <br><br>
    Here, it is clear that the number of iterations that this algorithm is run for makes 
    a considerable difference in the amount of refinement of the output image. With more 
    iterations, more class tiling occurs and the image becomes less like a flamingo or a 
    swan in terms of shape and structure, yet more like a flamingo or swan in terms of 
    overall texture.
    
    <br><br>
    <b>Computational biology application</b><br>
    A difficult problem in computational drug discovery is automatically finding small 
    drugs that will strongly bind to a target of interest and block the activity of the 
    protein. While there is a wealth of information regarding chemical compounds online, 
    it is difficult to search these large databases and intelligently select only a few 
    top candidates to move on for clinical trials. If a drug can be modeled as a 
    3-dimensional “image” and fed into a CNN, perhaps it can benefit from the methods 
    described. That is, a CNN can use several labeled training examples of binding 
    and non-binding small molecules (for a specific protein target) to distinguish with 
    high accuracy the differences between a good (strong binding) drug and bad 
    (non-binding) drug. Then, when given the input of a drug that we already know is 
    good, this pre-trained CNN can transform it into an even better drug using the 
    activity maximization approach. <br><br>
    In this scenario, it is clear that different constraints will be needed (as opposed 
    to “neighboring pixels must be correlated”). Rather, concepts such as: atoms may not 
    be too close to each other in space, or, bonds can only twist and stretch so much, 
    must be conveyed through the implementation. Regardless, activity maximization for 
    drug discovery is an interesting future direction to explore.
    
    
    
    
  </p>
    
    

    </body>
</html>